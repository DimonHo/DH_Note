**以下仅针对5.x聚合解释说明**  
Example:
```
{
    "aggs" : {
        "genres" : {
            "terms" : { "field" : "genre" }
        }
    }
}
```
返回结果
```
{
    ...

    "aggregations" : {
        "genres" : {
            "doc_count_error_upper_bound": 0, 
            "sum_other_doc_count": 0, 
            "buckets" : [ 
                {
                    "key" : "jazz",
                    "doc_count" : 10
                },
                {
                    "key" : "rock",
                    "doc_count" : 10
                },
                {
                    "key" : "electronic",
                    "doc_count" : 10
                },
            ]
        }
    }
}
```

上面我们注意到5.x的比之前返回的结果中多了两个参数"doc_count_error_upper_bound" 和 "sum_other_doc_count"。
简单解释以下这两个参数：先说`sum_other_doc_count`,这个很简单，告诉你**所有分片**上还有多少个聚合的结果没有返回。注意，这不是统计最终有多少个聚合结果。

下面重点说下`doc_count_error_upper_bound`,这个参数的意思就是告诉我们统计的doc_count可能不准确，doc_count_error_upper_bound是最大误差数。
为什么会有误差？
因为es是分布式的啊。
我们先来看看上面这个聚合请求干了什么事：
1. 首先它会在每个分片上进行聚合排序，默认从每个分片上取出排在前面的10个结果。
2. 每个分片将前面这10个结果返回给主节点，主节点在将相同的结果进行相加后排序。
3. 返回给用户最终排序后的10个结果。
关键就在第一步，比如返回结果中

| 分片1 | 分片2 | 分片3 |
| :---- |:-----| :-----|
| a[20] | b[19]| f[45] |
| v[19] | a[4] | a[25] |
| b[18] | c[4] | g[23] |
| c[3]  | d[3] | b[11] |
| f[1]  | g[1] | c[3]  |

现在假设我们把size设为2（默认为10），只返回2个解结果，那么先从每个分片取出前两个,然后相加得到排序结果：  

| 分片1 | 分片2  |分片3  |  
| :---- |:----- | :-----|  
| a[20] | b[19] | f[45] |  
| v[19] | a[4]  | a[25] |  

a = 20+4+25 = 49  
b = 19  
f = 45  
v = 19  
很显然，排序后将返回a和f   

|结果 |  
|:----|
|a[49]|
|f[45]|

但是，事实上这个排序结果是不正确的，因为从所有的数据来看，b=18+19+11=48,实际b才应该是排第2的。  
如果你把的size设为4，你将得到这样的结果:  

|结果 |
|:--  |
|a[49]|
|b[48]|
|f[45]|
|g[23]|

所以，如果size=2，最坏的情况下doc_count_error_upper_bound = 19+4+25 = 48 。  
如果想看具体某一个key的最坏解果，可以添加`show_term_doc_count_error:true`参数。  
比如上面size=2的时候，f的最坏结果是分片1和分片2的结果数没能返回，所以f的最坏结果数是doc_count_error_upper_bound=19+4=23，  
而a所有的分片都返回了结果，所以a的doc_count_error_upper_bound=0  


如果理解了上面的原理的话，那么如果想减少误差怎么做呢，很明显，默认的shard_size是等于size的，即每个分片返回的结果数量等于size,如果我们让每个分片返回的结果数多一点，那么这个误差也就能够变的小一点了。通过设置shard_size就能办到这一点，注意，即使你把shard_size设置的小于size,默认也会自动帮你修正为等于size的。 
```json
POST wos_source2.0/periodical/_search
{
  "aggs": {
    "res": {
      "terms": {
        "field": "keywords.full",
        "size": 10, 
        "shard_size": 100, 
        "show_term_doc_count_error": true
      }
    }
  }
}
```



参考：http://baijiahao.baidu.com/s?id=1567492735160834
